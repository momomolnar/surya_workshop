{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d651fb69",
   "metadata": {},
   "source": [
    "# PyTorch dataset template\n",
    "\n",
    "by Andr√©s Mu√±oz-Jaramillo\n",
    "\n",
    "This notebook is meant to act as a template to create a custom dataset based on a downstream application (DS) index.\n",
    "\n",
    "It requires an DS index file to be combined with a HelioFM index.  It also shows how to create a child database class based on HelioFM's database class so that all the code related to the input data is handled transparently, while the new code focuses exclusively in adding the DS information\n",
    "\n",
    "This template uses a flare forecasting dataset as an example, casting the problem as an X-ray flux regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed04321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import sunpy.visualization.colormaps as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Append base path.  May need to be modified if the folder structure changes.\n",
    "# It gives the notebook access to the wokshop_infrastructure folder.\n",
    "sys.path.append(\"../../\")\n",
    " \n",
    "# Append Surya path. May need to be modified if the folder structure changes.\n",
    "# It gives the notebook access to surya's release code.\n",
    "sys.path.append(\"../../Surya\")\n",
    "\n",
    "from surya.utils.data import build_scalers  # Data scaling utilities for Surya stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e9110",
   "metadata": {},
   "source": [
    "## Download scalers\n",
    "Surya input data needs to be scaled properly for the model to work and this cell downloads the scaling information.\n",
    "\n",
    "- If the cell below fails, try running the provided shell script directly in the terminal.\n",
    "- Sometimes the download may fail due to network or server issues‚Äîif that happens, simply re-run the script a few times until it completes successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b87776",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh download_scalers.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04135ff7",
   "metadata": {},
   "source": [
    "## Load configuration\n",
    "\n",
    "Surya was designed to read a configuration file that defines many aspects of the model\n",
    "including the data it uses we use this config file to set default values that do not\n",
    "need to be modified, but also to define values specific to our downstream application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration paths - modify these if your files are in different locations\n",
    "config_path = \"./configs/config.yaml\"\n",
    "\n",
    "# Load configuration\n",
    "print(\"üìã Loading configuration...\")\n",
    "try:\n",
    "    config = yaml.safe_load(open(config_path, \"r\"))\n",
    "    config[\"data\"][\"scalers\"] = yaml.safe_load(open(config[\"data\"][\"scalers_path\"], \"r\"))\n",
    "    print(\"‚úÖ Configuration loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Make sure config.yaml exists in your current directory\")\n",
    "    raise\n",
    "\n",
    "scalers = build_scalers(info=config[\"data\"][\"scalers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6574dc",
   "metadata": {},
   "source": [
    "## Define DS dataset\n",
    "\n",
    "This child class takes as input all expected HelioFM parameters, plus additonal parameters relevant to the downstream application.  Here we focus in particular to the DS index and parameters necessary to combine it with the HelioFM index.\n",
    "\n",
    "Another important component of creating a dataset class for your DS is normalization.  Here we use a log normalization on xray flux that will act as the output target.  Making log10(xray_flux) strictly positive and having 66% of its values between 0 and 1\n",
    "\n",
    "Since we are going to use this dataset moving forward, it is better to develop it as script and not as a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from downstream_apps.template.datasets.template_dataset import FlareDSDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319a7e4",
   "metadata": {},
   "source": [
    "## Initialize class without Surya stacks\n",
    "\n",
    "All the parameters that define a HelioFM dataset are contained within the test config file.  Scalers used to normalize HelioFm's input data are also necessary\n",
    "\n",
    "**_Important: This first initalization is set not to return the full Surya stack so that we can verify that the target is returning what we expect. This is so that you can check things quickly without having to pull full stacks until you need them._**\n",
    "\n",
    "_We do this by setting return_surya_stack=False_\n",
    "\n",
    "Make sure to set return_surya_stack=True if you need the full surya stack at this stage, otherwise we'll reinitialize the dataset shortly.\n",
    "\n",
    "**_Important:  In this notebook we sets max_number_of_samples=6 to potentially avoid going through the whole dataset as we explore it.  Keep in mind this for the future in case the database seems smaller than you expect_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FlareDSDataset(\n",
    "    #### All these lines are required by the parent HelioNetCDFDataset class\n",
    "    index_path=config[\"data\"][\"train_data_path\"],\n",
    "    time_delta_input_minutes=config[\"data\"][\"time_delta_input_minutes\"],\n",
    "    time_delta_target_minutes=config[\"data\"][\"time_delta_target_minutes\"],\n",
    "    n_input_timestamps=config[\"model\"][\"time_embedding\"][\"time_dim\"],\n",
    "    rollout_steps=config[\"rollout_steps\"],\n",
    "    channels=config[\"data\"][\"channels\"],\n",
    "    drop_hmi_probability=config[\"drop_hmi_probablity\"],\n",
    "    use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
    "    scalers=scalers,\n",
    "    phase=\"train\",\n",
    "    s3_use_simplecache = True,\n",
    "    s3_cache_dir= \"/tmp/helio_s3_cache\",    \n",
    "    #### Put your donwnstream (DS) specific parameters below this line\n",
    "    return_surya_stack=False,\n",
    "    max_number_of_samples=6,\n",
    "    ds_flare_index_path=\"./data/hek_flare_catalog.csv\",\n",
    "    ds_time_column=\"start_time\",\n",
    "    ds_time_tolerance = \"4d\",\n",
    "    ds_match_direction = \"forward\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82762d57",
   "metadata": {},
   "source": [
    "## Test length and structure\n",
    "\n",
    "Now we can test that the database is properly initialized and returns what is expected. In th case of the template, there are 6294 flares that take place during the training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = train_dataset.__getitem__(0)\n",
    "item.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09615b0",
   "metadata": {},
   "source": [
    "Note that the dataset returns a single item and it's always the same item according to the index used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2287e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cab5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed680710",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719afb0d",
   "metadata": {},
   "source": [
    "## Define dataloader\n",
    "\n",
    "With a working dataset we can define a dataloader.  A dataloader is simply a wrapper around a dataset that includes a sampling strategy to turn your dataset into batches.   Once we request a batch, the dataloader will return a dictionary like the dataset, but data inside will have a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=5,\n",
    "                num_workers=8\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399c8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dad7b0",
   "metadata": {},
   "source": [
    "Now the batch will have more than one item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bce301",
   "metadata": {},
   "source": [
    "Typically we set dataloaders to shufle the data so that the model sees data in different order during training.  This means that in general we don't want and don't expect the batch to return the same sequence of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3acdc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=5,\n",
    "                shuffle=True,\n",
    "            )\n",
    "\n",
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307aaa56",
   "metadata": {},
   "source": [
    "## Initialize class with Surya stacks\n",
    "\n",
    "Now we initalize the database to return full surya stacks to visualize them by setting _return_surya_stack=True_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FlareDSDataset(\n",
    "    #### All these lines are required by the parent HelioNetCDFDataset class\n",
    "    index_path=config[\"data\"][\"train_data_path\"],\n",
    "    time_delta_input_minutes=config[\"data\"][\"time_delta_input_minutes\"],\n",
    "    time_delta_target_minutes=config[\"data\"][\"time_delta_target_minutes\"],\n",
    "    n_input_timestamps=config[\"model\"][\"time_embedding\"][\"time_dim\"],\n",
    "    rollout_steps=config[\"rollout_steps\"],\n",
    "    channels=config[\"data\"][\"channels\"],\n",
    "    drop_hmi_probability=config[\"drop_hmi_probablity\"],\n",
    "    use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
    "    scalers=scalers,\n",
    "    phase=\"train\",\n",
    "    s3_use_simplecache = True,\n",
    "    s3_cache_dir= \"/tmp/helio_s3_cache\",    \n",
    "    #### Put your donwnstream (DS) specific parameters below this line\n",
    "    return_surya_stack=True,\n",
    "    max_number_of_samples=6,\n",
    "    ds_flare_index_path=\"./data/hek_flare_catalog.csv\",\n",
    "    ds_time_column=\"start_time\",\n",
    "    ds_time_tolerance = \"4d\",\n",
    "    ds_match_direction = \"forward\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18403f11",
   "metadata": {},
   "source": [
    "In surya's convention an item contains the following elements:\n",
    "- 'ts': input tensor.\n",
    "- 'time_delta_input': minutes with respect to the present in the time dimension of the input tensor.\n",
    "- 'forecast': target SDO stack.\n",
    "- 'lead_time_delta': how many minutes into the future is the target stack with respect to the present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9108229",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = train_dataset.__getitem__(0)\n",
    "item.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba5cc8",
   "metadata": {},
   "source": [
    "The shape of the input tensors has dimensions [C, T, H, W], where\n",
    "- C: instrument channels.\n",
    "- T: timestamps.\n",
    "- H: Height.\n",
    "- W: Width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "item['ts'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0731f",
   "metadata": {},
   "source": [
    "## Plotting input stack\n",
    "\n",
    "Before plotting, it is necessary to undo the z-score and logarithmic normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_ts = train_dataset.inverse_transform_data(item['ts'][:,0,...])\n",
    "channel_order = config[\"data\"][\"channels\"]\n",
    "fig = plt.figure(figsize=np.array([4,4]), dpi=300)\n",
    "gs = gridspec.GridSpec(4, 4, figure=fig, wspace=0, hspace=0)\n",
    "\n",
    "limits = {}\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        n = i*4 + j\n",
    "        if n < len(channel_order):\n",
    "\n",
    "            ax = fig.add_subplot(gs[i,j])\n",
    "            channel = channel_order[n]\n",
    "            if 'hmi' not in channel:\n",
    "                lim = np.percentile(unnormalized_ts[n,...][unnormalized_ts[n,...]!=0], 99)\n",
    "                ax.imshow(unnormalized_ts[n,...], cmap=f'sdo{channel}', vmin=0, vmax=lim)\n",
    "                font_color = 'w'\n",
    "\n",
    "            else:\n",
    "                font_color = 'k'\n",
    "                if \"_v\" not in channel:\n",
    "                    lim = 1000\n",
    "                    ax.imshow(unnormalized_ts[n,...], cmap=f'hmimag', vmin = -lim, vmax=lim)\n",
    "                else:\n",
    "                    lim = 1000\n",
    "                    ax.imshow(unnormalized_ts[n,...], cmap=f'coolwarm', vmin = -lim, vmax=lim)                    \n",
    "\n",
    "            ax.text(0.01, 0.99, channel, transform=ax.transAxes, horizontalalignment='left', verticalalignment='top', color=font_color, fontsize=5)  \n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13f88b",
   "metadata": {},
   "source": [
    "## Define dataloader\n",
    "\n",
    "Now the dataloader will return also a surya stack alongside our flaring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc696b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=2\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3716611",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92a13a",
   "metadata": {},
   "source": [
    "And now it will also have a batch dimension of 2 (batch dimensions are typically the leftmost dimension in a tensor's shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['ts'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b56b26e",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Once this notebook runs successfully you have a dataset and dataloaders done and ready to train a DS application. The next step continues in 1_baseline_template.ipynb which will involve putting together a simple baseline including metrics and a training loop that can be used to compare with Surya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f76e5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surya_ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
