{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d651fb69",
   "metadata": {},
   "source": [
    "# PyTorch dataset template\n",
    "\n",
    "This notebook is meant to act as a template to create a custom dataset based on a downstream application (DS) index.\n",
    "\n",
    "It requires an DS index file to be combined with a HelioFM index.  It also shows how to create a child database class based on HelioFM's database class so that all the code related to the input data is handled transparently, while the new code focuses exclusively in adding the DS information\n",
    "\n",
    "This template uses a flare forecasting dataset as an example, casting the problem as an X-ray flux regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ed04321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "# Append base path.  May need to be modified if the folder structure changes\n",
    "sys.path.append(\"../../HelioFM\")\n",
    "from train_spectformer import get_config\n",
    "from utils.data import build_scalers\n",
    "from datasets.helio import HelioNetCDFDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d5ba7",
   "metadata": {},
   "source": [
    "### Define DS dataset\n",
    "\n",
    "This child class takes as input all expected HelioFM parameters, plus additonal parameters relevant to the downstream application.  Here we focus in particular to the DS index and parameters necessary to combine it with the HelioFM index.\n",
    "\n",
    "Another important component of creating a dataset class for your DS is normalization.  Here we use a log normalization on xray flux that will act as the output target.  Making log10(xray_flux) strictly positive and having 66% of its values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c82fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlareDSDataset(HelioNetCDFDataset):\n",
    "    \"\"\"\n",
    "    Template child class of HelioNetCDFDataset to show an example of how to create a\n",
    "    dataset for donwstream applications. It includes both the necessary parameters\n",
    "    to initialize the parent class, as well as those of the child\n",
    "\n",
    "    HelioFM Parameters\n",
    "    ------------------\n",
    "    index_path : str\n",
    "        Path to HelioFM index\n",
    "    time_delta_input_minutes : list[int]\n",
    "        Input delta times to define the input stack in minutes from the present\n",
    "    time_delta_target_minutes : int\n",
    "        Target delta time to define the output stack on rollout in minutes from the present\n",
    "    n_input_timestamps : int\n",
    "        Number of input timestamps\n",
    "    rollout_steps : int\n",
    "        Number of rollout steps\n",
    "    scalers : optional\n",
    "        scalers used to perform input data normalization, by default None\n",
    "    num_mask_aia_channels : int, optional\n",
    "        Number of aia channels to mask during training, by default 0\n",
    "    drop_hmi_probablity : int, optional\n",
    "        Probability of removing hmi during training, by default 0\n",
    "    use_latitude_in_learned_flow : bool, optional\n",
    "        Switch to provide heliographic latitude for each datapoint, by default False\n",
    "    channels : list[str] | None, optional\n",
    "        Input channels to use, by default None\n",
    "    phase : str, optional\n",
    "        Descriptor of the phase used for this database, by default \"train\"\n",
    "\n",
    "    Downstream (DS) Parameters\n",
    "    --------------------------\n",
    "    ds_flare_index_path : str, optional\n",
    "        DS index.  In this example a flare dataset, by default None\n",
    "    ds_time_column : str, optional\n",
    "        Name of the column to use as datestamp to compare with HelioFM's index, by default None\n",
    "    ds_time_tolerance : str, optional\n",
    "        How much time difference is tolerated when finding matches between HelioFM and the DS, by default None\n",
    "    ds_match_direction : str, optional\n",
    "        Direction used to find matches using pd.merge_asof possible values are \"forward\", \"backward\",\n",
    "        or \"nearest\".  For causal relationships is better to use \"forward\", by default \"forward\"\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        Error is raised if there is not overlap between the HelioFM and DS indices\n",
    "        given a tolerance\n",
    "    \"\"\" \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #### All these lines are required by the parent HelioNetCDFDataset class \n",
    "        index_path: str,\n",
    "        time_delta_input_minutes: list[int],\n",
    "        time_delta_target_minutes: int,\n",
    "        n_input_timestamps: int,\n",
    "        rollout_steps: int,\n",
    "        scalers=None,\n",
    "        num_mask_aia_channels=0,\n",
    "        drop_hmi_probablity=0,\n",
    "        use_latitude_in_learned_flow=False,\n",
    "        channels: list[str] | None = None,\n",
    "        phase=\"train\",\n",
    "        #### Put your donwnstream (DS) specific parameters below this line\n",
    "        ds_flare_index_path: str = None,\n",
    "        ds_time_column: str = None,\n",
    "        ds_time_tolerance: str = None,\n",
    "        ds_match_direction: str = \"forward\"\n",
    "    ):       \n",
    "          \n",
    "        ## Initialize parent class\n",
    "        super().__init__(\n",
    "            index_path=index_path,\n",
    "            time_delta_input_minutes=time_delta_input_minutes,\n",
    "            time_delta_target_minutes=time_delta_target_minutes,\n",
    "            n_input_timestamps=n_input_timestamps,\n",
    "            rollout_steps=rollout_steps,\n",
    "            scalers=scalers,\n",
    "            num_mask_aia_channels=num_mask_aia_channels,\n",
    "            drop_hmi_probablity=drop_hmi_probablity,\n",
    "            use_latitude_in_learned_flow=use_latitude_in_learned_flow,\n",
    "            channels=channels,\n",
    "            phase=phase,\n",
    "        )\n",
    "\n",
    "        # Load ds index and find intersection with HelioFM index\n",
    "        self.ds_index = pd.read_csv(ds_flare_index_path)\n",
    "        self.ds_index[\"ds_index\"] = pd.to_datetime(self.ds_index[ds_time_column]).values.astype(\n",
    "            \"datetime64[ns]\"\n",
    "        )\n",
    "        self.ds_index.sort_values(\"ds_index\", inplace=True)\n",
    "\n",
    "        # Implement normalization.  This is going to be DS application specific, no two will look the same\n",
    "        self.ds_index[\"normalized_intensity\"] = np.log10(self.ds_index[\"intensity\"])\n",
    "        self.ds_index[\"normalized_intensity\"] = self.ds_index[\"normalized_intensity\"] - np.min(self.ds_index[\"normalized_intensity\"])\n",
    "        self.ds_index[\"normalized_intensity\"] = self.ds_index[\"normalized_intensity\"]/(2*np.std(self.ds_index[\"normalized_intensity\"]))\n",
    "\n",
    "        # Create HelioFM valid indices and find closest match to DS index \n",
    "        self.df_valid_indices = pd.DataFrame({\"valid_indices\":self.valid_indices}).sort_values(\"valid_indices\")\n",
    "        self.df_valid_indices = pd.merge_asof(self.df_valid_indices, self.ds_index, right_on=\"ds_index\", left_on=\"valid_indices\", direction=ds_match_direction)\n",
    "        # Remove duplicates keeping closest match\n",
    "        self.df_valid_indices[\"index_delta\"] = np.abs(self.df_valid_indices[\"valid_indices\"] - self.df_valid_indices[\"ds_index\"])\n",
    "        self.df_valid_indices = self.df_valid_indices.sort_values([\"ds_index\", \"index_delta\"])\n",
    "        self.df_valid_indices.drop_duplicates(subset=\"ds_index\", keep=\"first\", inplace=True)\n",
    "        # Enforce a maximum time tolerance for matches\n",
    "        if ds_time_tolerance is not None:\n",
    "            self.df_valid_indices = self.df_valid_indices.loc[self.df_valid_indices['index_delta']<=pd.Timedelta(ds_time_tolerance),:]\n",
    "            if len(self.df_valid_indices) == 0:\n",
    "                raise ValueError(\"No intersection between HelioFM and DS indices\")\n",
    "\n",
    "        # Override valid indices variables to reflect matches between HelioFM and DS\n",
    "        self.valid_indices = [pd.Timestamp(date) for date in self.df_valid_indices[\"valid_indices\"]]\n",
    "        self.adjusted_length = len(self.valid_indices)\n",
    "        self.df_valid_indices.set_index(\"valid_indices\", inplace=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.adjusted_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: Index of sample to load. (Pytorch standard.)\n",
    "        Returns:\n",
    "            Dictionary with following keys. The values are tensors with shape as follows:\n",
    "                # HelioFM keys--------------------------------\n",
    "                ts (torch.Tensor):                C, T, H, W\n",
    "                time_delta_input (torch.Tensor):  T\n",
    "                input_latitude (torch.Tensor):    T\n",
    "                forecast (torch.Tensor):          C, L, H, W\n",
    "                lead_time_delta (torch.Tensor):   L\n",
    "                forecast_latitude (torch.Tensor): L\n",
    "                # HelioFM keys--------------------------------\n",
    "                flare_intensity_target\n",
    "            C - Channels, T - Input times, H - Image height, W - Image width, L - Lead time.\n",
    "        \"\"\"\n",
    "\n",
    "        # This lines assembles the dictionary that HelioFM's dataset returns (defined above)\n",
    "        base_dictionary, metadata = super().__getitem__(idx=idx)\n",
    "\n",
    "        # We now add the flare intensity label\n",
    "        base_dictionary['target'] = self.df_valid_indices.iloc[idx][\"normalized_intensity\"]\n",
    "\n",
    "        return base_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319a7e4",
   "metadata": {},
   "source": [
    "### Initialize class\n",
    "\n",
    "All the parameters that define a HelioFM dataset are contained within the test config file.  Scalers used to normalize HelioFm's input data are also necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b658dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config_spectformer_dgx_test.yaml\"\n",
    "config = get_config(config_path)\n",
    "scalers = build_scalers(info=config.data.scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f76d377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FlareDSDataset(\n",
    "    #### All these lines are required by the parent HelioNetCDFDataset class\n",
    "    index_path=config.data.train_data_path,\n",
    "    time_delta_input_minutes=config.data.time_delta_input_minutes,\n",
    "    time_delta_target_minutes=config.data.time_delta_target_minutes,\n",
    "    n_input_timestamps=config.data.n_input_timestamps,\n",
    "    rollout_steps=config.rollout_steps,\n",
    "    channels=config.data.channels,\n",
    "    drop_hmi_probablity=config.drop_hmi_probablity,\n",
    "    num_mask_aia_channels=config.num_mask_aia_channels,\n",
    "    use_latitude_in_learned_flow=config.use_latitude_in_learned_flow,\n",
    "    scalers=scalers,\n",
    "    phase=\"train\",\n",
    "    #### Put your donwnstream (DS) specific parameters below this line\n",
    "    ds_flare_index_path=\"../data/hek_flare_catalog.csv\",\n",
    "    ds_time_column=\"start_time\",\n",
    "    ds_time_tolerance = \"4d\",\n",
    "    ds_match_direction = \"forward\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82762d57",
   "metadata": {},
   "source": [
    "### Test length and return\n",
    "\n",
    "Now we can test that the database is properly initialized and returns what is expected.  Using the toy dataset with a single day it should have a length of 1 and return a dictionary including the DS label \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "677d7c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9108229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ts', 'time_delta_input', 'forecast', 'lead_time_delta', 'target'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = train_dataset.__getitem__(0)\n",
    "item.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13f88b",
   "metadata": {},
   "source": [
    "### Define dataloader\n",
    "\n",
    "With a working dataset we can define a dataloader.  A dataloader is simply a wrapper around a dataset that includes a sampling strategy to turn your dataset into batches.   Once we request a batch, the dataloader will return a dictionary like the dataset, but data inside will have a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffc696b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3716611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ts', 'time_delta_input', 'forecast', 'lead_time_delta', 'target'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "368b242f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 2, 4096, 4096])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['ts'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d2dea",
   "metadata": {},
   "source": [
    "### Define simple baseline model\n",
    "\n",
    "Defining a simple baseline is important to understand what value is bringing the AI model to the problem.  In this example we define a simple logistic regression acting on the intensity of each channel.  Note that we invert the normalization to deal with strictly positive quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f5fd639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFlareModel(nn.Module):\n",
    "    def __init__(self, input_dim, channel_order, scalers):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.channel_order = channel_order\n",
    "        self.scalers = scalers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get dimensions\n",
    "        b, c, t, w, h = x.shape\n",
    "\n",
    "        # Invert normalization\n",
    "        for channel_index, channel in enumerate(self.channel_order):\n",
    "            x[:, channel_index, ...] = self.scalers[channel].inverse_transform(\n",
    "                x[:, channel_index, ...]\n",
    "            )\n",
    "\n",
    "        # Collapse input stack spatially\n",
    "        x = x.abs().mean(dim=[3,4])\n",
    "\n",
    "        # Rearange in preparation for linear layer\n",
    "        x = rearrange(x, \"b c t -> b (c t)\")\n",
    "\n",
    "        out = torch.sigmoid(self.linear(x))\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5013b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, c, t, w, h = batch['ts'].shape\n",
    "model = SimpleFlareModel(c*t, config.data.channels, scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7d97841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9977]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(batch['ts'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
