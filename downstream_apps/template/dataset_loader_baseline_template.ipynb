{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d651fb69",
   "metadata": {},
   "source": [
    "# PyTorch dataset template\n",
    "\n",
    "This notebook is meant to act as a template to create a custom dataset based on a downstream application (DS) index.\n",
    "\n",
    "It requires an DS index file to be combined with a HelioFM index.  It also shows how to create a child database class based on HelioFM's database class so that all the code related to the input data is handled transparently, while the new code focuses exclusively in adding the DS information\n",
    "\n",
    "This template uses a flare forecasting dataset as an example, casting the problem as an X-ray flux regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed04321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import yaml\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "# Append base path.  May need to be modified if the folder structure changes.\n",
    "# It gives the notebook access to the wokshop_infrastructure folder.\n",
    "sys.path.append(\"../../\")\n",
    " \n",
    "# Append Surya path. May need to be modified if the folder structure changes.\n",
    "# It gives the notebook access to surya's release code.\n",
    "sys.path.append(\"../../Surya\")\n",
    "# from train_spectformer import get_config\n",
    "from surya.utils.data import build_scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d5ba7",
   "metadata": {},
   "source": [
    "### Define DS dataset\n",
    "\n",
    "This child class takes as input all expected HelioFM parameters, plus additonal parameters relevant to the downstream application.  Here we focus in particular to the DS index and parameters necessary to combine it with the HelioFM index.\n",
    "\n",
    "Another important component of creating a dataset class for your DS is normalization.  Here we use a log normalization on xray flux that will act as the output target.  Making log10(xray_flux) strictly positive and having 66% of its values between 0 and 1\n",
    "\n",
    "Since we are going to use this dataset moving forward, it is better to develop it as script and not as a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from downstream_apps.template.datasets.template_dataset import FlareDSDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e9110",
   "metadata": {},
   "source": [
    "### Download scalers\n",
    "Surya input data needs to be scaled properly for the model to work and this cell downloads the scaling information.\n",
    "\n",
    "- If the cell below fails, try running the provided shell script directly in the terminal.\n",
    "- Sometimes the download may fail due to network or server issuesâ€”if that happens, simply re-run the script a few times until it completes successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b87776",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh download_scalers.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04135ff7",
   "metadata": {},
   "source": [
    "### Load configuration\n",
    "\n",
    "Surya was designed to read a configuration file that defines many aspects of the model\n",
    "including the data it uses we use this config file to set default values that do not\n",
    "need to be modified, but also to define values specific to our downstream application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration paths - modify these if your files are in different locations\n",
    "config_path = \"./configs/config.yaml\"\n",
    "\n",
    "# Load configuration\n",
    "print(\"ðŸ“‹ Loading configuration...\")\n",
    "try:\n",
    "    config = yaml.safe_load(open(config_path, \"r\"))\n",
    "    config[\"data\"][\"scalers\"] = yaml.safe_load(open(config[\"data\"][\"scalers_path\"], \"r\"))\n",
    "    print(\"âœ… Configuration loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"Make sure config.yaml exists in your current directory\")\n",
    "    raise\n",
    "\n",
    "scalers = build_scalers(info=config[\"data\"][\"scalers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319a7e4",
   "metadata": {},
   "source": [
    "### Initialize class\n",
    "\n",
    "All the parameters that define a HelioFM dataset are contained within the test config file.  Scalers used to normalize HelioFm's input data are also necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FlareDSDataset(\n",
    "    #### All these lines are required by the parent HelioNetCDFDataset class\n",
    "    index_path=config[\"data\"][\"train_data_path\"],\n",
    "    time_delta_input_minutes=config[\"data\"][\"time_delta_input_minutes\"],\n",
    "    time_delta_target_minutes=config[\"data\"][\"time_delta_target_minutes\"],\n",
    "    n_input_timestamps=config[\"model\"][\"time_embedding\"][\"time_dim\"],\n",
    "    rollout_steps=config[\"rollout_steps\"],\n",
    "    channels=config[\"data\"][\"channels\"],\n",
    "    drop_hmi_probability=config[\"drop_hmi_probablity\"],\n",
    "    use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
    "    scalers=scalers,\n",
    "    phase=\"train\",\n",
    "    #### Put your donwnstream (DS) specific parameters below this line\n",
    "    ds_flare_index_path=\"./data/hek_flare_catalog.csv\",\n",
    "    ds_time_column=\"start_time\",\n",
    "    ds_time_tolerance = \"4d\",\n",
    "    ds_match_direction = \"forward\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82762d57",
   "metadata": {},
   "source": [
    "### Test length and return\n",
    "\n",
    "Now we can test that the database is properly initialized and returns what is expected.  Using the toy dataset with a single day it should have a length of 1 and return a dictionary including the DS label \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9108229",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = train_dataset.__getitem__(0)\n",
    "item.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13f88b",
   "metadata": {},
   "source": [
    "### Define dataloader\n",
    "\n",
    "With a working dataset we can define a dataloader.  A dataloader is simply a wrapper around a dataset that includes a sampling strategy to turn your dataset into batches.   Once we request a batch, the dataloader will return a dictionary like the dataset, but data inside will have a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc696b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3716611",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['ts'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d2dea",
   "metadata": {},
   "source": [
    "### Define simple baseline model\n",
    "\n",
    "Defining a simple baseline is important to understand what value is bringing the AI model to the problem.  In this example we define a simple logistic regression acting on the intensity of each channel.  Note that we invert the normalization to deal with strictly positive quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5fd639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFlareModel(nn.Module):\n",
    "    def __init__(self, input_dim, channel_order, scalers):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.channel_order = channel_order\n",
    "        self.scalers = scalers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get dimensions\n",
    "        b, c, t, w, h = x.shape\n",
    "\n",
    "        # Invert normalization\n",
    "        for channel_index, channel in enumerate(self.channel_order):\n",
    "            x[:, channel_index, ...] = self.scalers[channel].inverse_transform(\n",
    "                x[:, channel_index, ...]\n",
    "            )\n",
    "\n",
    "        # Collapse input stack spatially\n",
    "        x = x.abs().mean(dim=[3,4])\n",
    "\n",
    "        # Rearange in preparation for linear layer\n",
    "        x = rearrange(x, \"b c t -> b (c t)\")\n",
    "\n",
    "        out = torch.sigmoid(self.linear(x))\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5013b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, c, t, w, h = batch['ts'].shape\n",
    "model = SimpleFlareModel(c*t, config.data.channels, scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d97841",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(batch['ts'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surya_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
